{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data Using PySpark Functionalities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databricks Data Management and Analysis\n",
    "\n",
    "Objective: Participants will implement a fully modular ETL pipeline using Python to interact with a Databricks workspace, load data, perform transformations, and generate insights.\n",
    "\n",
    "They must:\n",
    "\n",
    "Use Python functions to manage Databricks SQL operations (setup, data loading, transformations).\n",
    "Analyze data using Python for data analysis and visualization.\n",
    "Utilize PySpark for advanced transformations where applicable.\n",
    "Provide the solution in Python functions following the specified structure.\n",
    "Refrain from modifying any provided boilerplate code, as it may cause unexpected behavior.\n",
    "The solution should be implemented between the comments # code starts here and # code ends here.\n",
    "Before submission:\n",
    "\n",
    "Ensure there are no errors while executing the notebook.\n",
    "Set the kernel of the Jupyter notebook to Python 3.10.6 if it is not set already.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.mgmt.databricks import AzureDatabricksManagementClient\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from databricks import sql\n",
    "import subprocess\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_number = random.randint(1000, 9999)\n",
    "workspace_name = f\"demoworkspacesi{random_number}\"\n",
    "resource_group_name = \"Test\"\n",
    "location = \"eastus\"\n",
    "cluster_name = \"my-databricks-cluster\"\n",
    "managed_resource_group_name = \"test-rg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Azure Credentials\n",
    "\n",
    "Executes the process of loading Azure credentials from a JSON file and returning an authenticated credential object.\n",
    "\n",
    "This function reads the azure_credentials.json file (or a specified file path), extracts required authentication details (tenant_id, client_id, client_secret, and subscription_id), and validates their presence. It then authenticates using ClientSecretCredential, allowing secure access to Azure resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_azure_credentials(credentials_file: str = \"azure_credentials.json\"):\n",
    "    \"\"\"\n",
    "    Loads Azure credentials from a JSON file and returns an authenticated credential object.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to the JSON file containing Azure credentials.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - ClientSecretCredential object for authentication\n",
    "            - Subscription ID as a string\n",
    "    \"\"\"\n",
    "    # Ensure the file exists\n",
    "    if not os.path.exists(credentials_file):\n",
    "        raise FileNotFoundError(f\"The credentials file '{credentials_file}' does not exist.\")\n",
    "    \n",
    "    # Load credentials from the file\n",
    "    with open(credentials_file, \"r\") as file:\n",
    "        creds = json.load(file)\n",
    "    global tenant_id , client_id,client_secret,subscription_id\n",
    "    # Extract required fields\n",
    "    tenant_id = creds.get(\"tenant_id\")\n",
    "    client_id = creds.get(\"client_id\")\n",
    "    client_secret = creds.get(\"client_secret\")\n",
    "    subscription_id = creds.get(\"subscription_id\")\n",
    "    \n",
    "    # Validate required fields\n",
    "    if not all([tenant_id, client_id, client_secret, subscription_id]):\n",
    "        raise ValueError(\"The credentials file is missing one or more required fields: 'tenant_id', 'client_id', 'client_secret', 'subscription_id'.\")\n",
    "    \n",
    "    # Authenticate using ClientSecretCredential\n",
    "    credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)\n",
    "    \n",
    "    return credential, subscription_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "credential, subscription_id = load_azure_credentials(\"azure_credentials.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Databricks Workspace and Retrieving Access Token\n",
    "\n",
    "Executes the process of creating an Azure Databricks workspace and retrieving its access token.\n",
    "\n",
    "This function initializes the Azure Resource Management Client to ensure the resource group exists, then uses the Azure Databricks Management Client to create a Databricks workspace in the specified location. The workspace is configured with a managed resource group and a \"premium\" SKU. Once created, the function returns the workspace name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_databricks_workspace_and_get_token(\n",
    "    credential: credential,\n",
    "    subscription_id: str,\n",
    "    resource_group_name: str,\n",
    "    managed_resource_group_name: str,\n",
    "    workspace_name: str,\n",
    "    location: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Creates a Databricks workspace and returns the workspace URL and access token.\n",
    "\n",
    "    Args:\n",
    "        credential (DefaultAzureCredential): Azure DefaultAzureCredential.\n",
    "        subscription_id (str): Azure subscription ID.\n",
    "        resource_group_name (str): Name of the Azure resource group.\n",
    "        managed_resource_group_name (str): Managed resource group name for Databricks.\n",
    "        workspace_name (str): Name of the Databricks workspace.\n",
    "        location (str): Azure region for the workspace.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the workspace URL and access token.\n",
    "    \"\"\"\n",
    "    # Initialize Resource Management Client\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "\n",
    "    # Create the resource group if it does not exist\n",
    "    print(f\"Ensuring resource group '{resource_group_name}' exists in '{location}'...\")\n",
    "    resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {\"location\": location}\n",
    "    )\n",
    "    print(f\"Resource group '{resource_group_name}' is ready.\")\n",
    "\n",
    "    # Initialize Databricks Management Client\n",
    "    databricks_client = AzureDatabricksManagementClient(credential, subscription_id)\n",
    "\n",
    "    # Construct managed resource group ID\n",
    "    managed_resource_group_id = f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}-managed\"\n",
    "\n",
    "    # Create the Databricks workspace\n",
    "    print(f\"Creating Databricks workspace '{workspace_name}' in '{location}'...\")\n",
    "    workspace = databricks_client.workspaces.begin_create_or_update(\n",
    "        resource_group_name=resource_group_name,\n",
    "        workspace_name=workspace_name,\n",
    "        parameters={\n",
    "            \"location\": location,\n",
    "            \"managed_resource_group_id\": managed_resource_group_id,\n",
    "            \"sku\": {\"name\": \"premium\"}\n",
    "        }\n",
    "    ).result()\n",
    "\n",
    "    print(f\"Databricks workspace '{workspace_name}' created successfully.\")\n",
    "    # print(workspace.workspace_url)\n",
    "    # workspaceurl = workspace.workspace_url\n",
    "    # Retrieve the workspace URL\n",
    "    \n",
    " \n",
    "    return workspace_name\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring resource group 'Test' exists in 'eastus'...\n",
      "Resource group 'Test' is ready.\n",
      "Creating Databricks workspace 'demoworkspacesi4176' in 'eastus'...\n",
      "Databricks workspace 'demoworkspacesi4176' created successfully.\n"
     ]
    }
   ],
   "source": [
    "workspace_name = create_databricks_workspace_and_get_token(\n",
    "        credential,\n",
    "        subscription_id,\n",
    "        resource_group_name,\n",
    "        managed_resource_group_name,\n",
    "        workspace_name,\n",
    "        location\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving Databricks HTTP Path\n",
    "\n",
    "Fetches the Databricks workspace URL using the Azure Management API.\n",
    "Retrieves the Databricks workspace properties, including the workspace URL.\n",
    "Authenticates and generates an access token for secure API requests.\n",
    "Queries the available SQL Warehouses within the Databricks workspace.\n",
    "Extracts the HTTP path from the first available warehouse.\n",
    "Returns the Databricks server hostname and the HTTP path for database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_databricks_http_path(credentials_file=\"azure_credentials.json\"):\n",
    "    \"\"\"\n",
    "    Retrieves the Databricks HTTP Path for connection.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to Azure credentials file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Databricks Server Hostname, HTTP Path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Databricks workspace URL\n",
    "    workspace_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.Databricks/workspaces/{workspace_name}?api-version=2018-04-01\"\n",
    "\n",
    "    access_token = credential.get_token(\"https://management.azure.com/.default\").token\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(workspace_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve Databricks workspace: {response.text}\")\n",
    "\n",
    "    workspace_data = response.json()\n",
    "    databricks_host = workspace_data[\"properties\"][\"workspaceUrl\"]\n",
    "\n",
    "    # Get SQL Warehouses from Databricks\n",
    "    sql_endpoint_url = f\"https://{databricks_host}/api/2.0/sql/warehouses\"\n",
    "    db_access_token = credential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token  # Databricks token\n",
    "\n",
    "    db_headers = {\n",
    "        \"Authorization\": f\"Bearer {db_access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    sql_response = requests.get(sql_endpoint_url, headers=db_headers)\n",
    "    if sql_response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve SQL Warehouses: {sql_response.text}\")\n",
    "\n",
    "    warehouses = sql_response.json()[\"warehouses\"]\n",
    "    if not warehouses:\n",
    "        raise Exception(\"No SQL Warehouses found in Databricks.\")\n",
    "\n",
    "    warehouse = warehouses[0]  # Selecting the first available warehouse\n",
    "    http_path = warehouse[\"odbc_params\"][\"path\"]\n",
    "\n",
    "    return databricks_host, http_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adb-1384417228045474.14.azuredatabricks.net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/sql/1.0/warehouses/7e2d54601457ffa5'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databricks_url, databricks_http_path = get_databricks_http_path()\n",
    "print(databricks_url)\n",
    "databricks_http_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create token of the databricks using the below steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "databricks_token = \"dapidb4c14f01445af5da740009ea8b9d777-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Databricks Cluster\n",
    "\n",
    "Constructs the Databricks API endpoint for cluster creation.\n",
    "Formats the workspace URL to ensure proper API request formatting.\n",
    "Sets up authentication headers using the provided access token.\n",
    "Defines cluster configuration, including the name, Spark version, node type, and auto-termination settings.\n",
    "Sends a POST request to the Databricks API to create the cluster.\n",
    "Handles API responses, printing success messages or error details based on the response status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def create_databricks_cluster(workspaceurl, access_token: str):\n",
    "    \n",
    "    # Validate and format the workspace URL\n",
    "    \n",
    "    # API endpoint for cluster creation\n",
    "    api_endpoint = \"/api/2.1/clusters/create\"\n",
    "    api_url = workspaceurl.rstrip(\"/\") + api_endpoint  # Ensure no trailing slash in URL\n",
    "\n",
    "    # Headers for authentication\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # Cluster configuration\n",
    "    cluster_config = {\n",
    "        \"cluster_name\": \"democluster\",\n",
    "        \"spark_version\": \"16.1.x-scala2.12\",  # Replace with your required version\n",
    "        \"node_type_id\": \"Standard_DS3_v2\",   # Choose the appropriate node type\n",
    "        \"autotermination_minutes\": 30,\n",
    "        \"num_workers\": 2\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send POST request to create the cluster\n",
    "        response = requests.post(api_url, headers=headers, json=cluster_config)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Cluster created successfully!\")\n",
    "            print(\"Cluster Details:\", response.json())\n",
    "        else:\n",
    "            print(f\"Failed to create cluster. HTTP Status Code: {response.status_code}\")\n",
    "            print(\"Error Response:\", response.text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"An error occurred while connecting to Databricks API:\")\n",
    "        print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster created successfully!\n",
      "Cluster Details: {'cluster_id': '0204-170211-dcovtoor'}\n"
     ]
    }
   ],
   "source": [
    "access_token = databricks_token\n",
    "workspaceurl = \"https://\"+databricks_url\n",
    "#\"https://\"+databricks_url\n",
    "create_databricks_cluster(workspaceurl, access_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mongomock\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, desc, split, trim\n",
    "import snowflake.connector\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Mongo_Resources():\n",
    "\n",
    "    # Initialize mongomock client to simulate MongoDB\n",
    "    mongo_client = mongomock.MongoClient()\n",
    "    db = mongo_client[\"population_database\"]\n",
    "\n",
    "    titles_collection = db[\"peoples\"]\n",
    "    #credits_collection = db[\"credits\"]\n",
    "    \n",
    "    # Load Titles data\n",
    "    titles_df = pd.read_csv(\"population-vs-price.csv\")\n",
    "    titles_data = titles_df.to_dict(\"records\")\n",
    "    titles_collection.insert_many(titles_data)\n",
    "\n",
    "    # Load Credits data\n",
    "    # credits_df = pd.read_csv(\"Credits.csv\")\n",
    "    # credits_data = credits_df.to_dict(\"records\")\n",
    "    # credits_collection.insert_many(credits_data)\n",
    "\n",
    "      # Verify insertion by printing counts of documents in each collection\n",
    "    print(f\"Titles count: {titles_collection.count_documents({})}\")\n",
    "    #print(f\"Credits count: {credits_collection.count_documents({})}\")\n",
    "\n",
    "    return titles_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles count: 294\n"
     ]
    }
   ],
   "source": [
    "titles_collection = create_Mongo_Resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_mongodb(titles_collection):\n",
    "    titles_df= None\n",
    "    # code starts here\n",
    "    titles_df = pd.DataFrame(list(titles_collection.find({}, {\"_id\": 0})))\n",
    "    #credits_df = pd.DataFrame(list(credits_collection.find({}, {\"_id\": 0,\"rank\":1})))\n",
    "    # code ends here\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles Data:\n",
      "   rank           City    State Code  Population  price\n",
      "0   101     Birmingham  Alabama   AL    212247.0  162.9\n",
      "1   125     Huntsville  Alabama   AL    188226.0  157.7\n",
      "2   122         Mobile  Alabama   AL    194675.0  122.5\n",
      "3   114     Montgomery  Alabama   AL    200481.0  129.0\n",
      "4    64  Anchorage[19]   Alaska   AK    301010.0    NaN\n"
     ]
    }
   ],
   "source": [
    "titles_df = extract_data_from_mongodb(titles_collection)\n",
    "print(\"Titles Data:\")\n",
    "print(titles_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294 entries, 0 to 293\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   rank        294 non-null    int64  \n",
      " 1   City        294 non-null    object \n",
      " 2   State       294 non-null    object \n",
      " 3   Code        294 non-null    object \n",
      " 4   Population  293 non-null    float64\n",
      " 5   price       109 non-null    float64\n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 13.9+ KB\n"
     ]
    }
   ],
   "source": [
    "titles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing a Databricks SQL Connection\n",
    "\n",
    "Sets up connection details using the Databricks workspace URL, HTTP path, and access token.\n",
    "Establishes a connection to Databricks SQL using the provided credentials.\n",
    "Returns the connection object for executing SQL queries.\n",
    "Ensures the connection is closed after use to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_databricks_connection():\n",
    "    \"\"\"\n",
    "    Establish and return a Databricks SQL connection.\n",
    "    Ensure to close the connection after use.\n",
    "\n",
    "    Returns:\n",
    "        conn (sql.Connection): Databricks SQL connection object.\n",
    "    \"\"\"\n",
    "    # Set up connection details\n",
    "    DATABRICKS_SERVER_HOSTNAME = workspaceurl  # Replace with your Databricks workspace URL\n",
    "    DATABRICKS_HTTP_PATH = databricks_http_path # Replace with your warehouse's HTTP Path\n",
    "    DATABRICKS_ACCESS_TOKEN = access_token  # Replace with your access token\n",
    "\n",
    "    # Establish connection\n",
    "    conn = sql.connect(\n",
    "        server_hostname=DATABRICKS_SERVER_HOSTNAME,\n",
    "        http_path=DATABRICKS_HTTP_PATH,\n",
    "        access_token=DATABRICKS_ACCESS_TOKEN\n",
    "    )\n",
    "\n",
    "    return conn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<databricks.sql.client.Connection at 0x715cd607f640>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global conn\n",
    "conn = get_databricks_connection()\n",
    "conn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Initializing SparkSession for Local File Reading\n",
    "\n",
    "Creates and returns a SparkSession instance.\n",
    "Sets the application name to \"DatabricksUpload\".\n",
    "Enables interaction with Spark for data processing and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize SparkSession for local file reading\n",
    "def initialize_spark():\n",
    "    \"\"\"Returns a Spark session.\"\"\"\n",
    "    return SparkSession.builder.appName(\"DatabricksUpload\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/31 13:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CSV File into DataFrame\n",
    "\n",
    "Reads a CSV file from the specified path using Spark.\n",
    "Sets header=True to use the first row as column names.\n",
    "Enables inferSchema=True to automatically detect data types.\n",
    "Returns the loaded DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load CSV File into DataFrame\n",
    "def load_csv(csv_path):\n",
    "    df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csv_path = \"/home/labuser/Desktop/Project/population-vs-price.csv\"\n",
    "df = load_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Table in Databricks SQL\n",
    "\n",
    "Defines an SQL query to create a population_vs_price table if it does not already exist.\n",
    "Specifies column names and data types, including rank, city, state, code, population, and price.\n",
    "Executes the query using a database connection cursor.\n",
    "Ensures table creation for storing population and price data in Databricks SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn):\n",
    "    \"\"\"\n",
    "    Creates a table in Databricks SQL if it doesn't exist.\n",
    "    \"\"\"\n",
    "    create_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS population_vs_price (\n",
    "        rank INT,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        code STRING,\n",
    "        population STRING,\n",
    "        price STRING\n",
    "    )\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        \n",
    "        result = cursor.execute(create_query)\n",
    "\n",
    "    return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_table(conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting Data from Spark DataFrame into Databricks SQL Table\n",
    "\n",
    "Converts the Spark DataFrame to a Pandas DataFrame for easier SQL insertion.\n",
    "Defines an SQL INSERT query to add data into the population_vs_price table.\n",
    "Iterates over the rows of the Pandas DataFrame, inserting each row into the table.\n",
    "Executes the query using a database connection cursor.\n",
    "Returns a success message once the data has been successfully inserted into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(conn, df):\n",
    "    \"\"\"\n",
    "    Inserts data from a Spark DataFrame into the Databricks SQL table.\n",
    "    \"\"\"\n",
    "    # Convert Spark DataFrame to Pandas for easy SQL insertion\n",
    "    pandas_df = df.toPandas()\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO population_vs_price (rank, City, State, Code, Population, Price) \n",
    "    VALUES (?, ?, ?, ?, ?, ?)\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        for _, row in pandas_df.iterrows():\n",
    "            cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "    return \"Successfully Inserted\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = insert_data(conn, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Find the top 5 cities with highest median sales price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Cities with Highest Sales\n",
    "\n",
    "Defines an SQL query to fetch the City, State, and price from the population_vs_price table, excluding null prices.\n",
    "Orders the results by price in descending order to identify the top 5 most expensive cities.\n",
    "Executes the query using a database connection cursor.\n",
    "Fetches the query results and returns the list of the top 5 cities with the highest prices.\n",
    "Ensures the cursor is closed automatically after fetching the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_sales(conn):\n",
    "    \"\"\"\n",
    "    Finds the most and least expensive cities based on median sales price.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT City, State, price \n",
    "    FROM population_vs_price\n",
    "    WHERE price IS NOT NULL\n",
    "    ORDER BY price DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:  # Cursor opens here\n",
    "        cursor.execute(query)\n",
    "        highest = cursor.fetchall()  # ✅ Fetch results inside the block\n",
    "\n",
    "    return highest  # ✅ Return the result after closing the cursor automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(City='Cambridge', State='Massachusetts', price='null'), Row(City='Charleston', State='South Carolina', price='null'), Row(City='Thousand Oaks', State='California', price='null'), Row(City='North Charleston', State='South Carolina', price='null'), Row(City='West Palm Beach', State='Florida', price='null')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "highest_sales = highest_sales(conn)\n",
    "print(highest_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finding the Cities with Lowest Sales\n",
    "\n",
    "Defines an SQL query to fetch the City, State, and price from the population_vs_price table, excluding null prices.\n",
    "Orders the results by price in ascending order to identify the top 5 least expensive cities.\n",
    "Executes the query using a database connection cursor.\n",
    "Fetches the query results and returns the list of the top 5 cities with the lowest prices.\n",
    "Ensures the cursor is automatically closed after fetching the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_sales(conn):\n",
    "    \"\"\"\n",
    "    Finds the most and least expensive cities based on median sales price.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT City, State, \"price\" \n",
    "    FROM population_vs_price\n",
    "    WHERE \"price\" IS NOT NULL\n",
    "    ORDER BY \"price\" ASC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:  # Cursor opens here\n",
    "        cursor.execute(query)\n",
    "        lowest = cursor.fetchall()  # ✅ Fetch results inside the block\n",
    "\n",
    "\n",
    "    return lowest\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(City='Long Beach', State='California', price='price'), Row(City='Gilbert[20]', State='Arizona', price='price'), Row(City='Arlington', State='Texas', price='price'), Row(City='Cincinnati', State='Ohio', price='price'), Row(City='Aurora', State='Colorado', price='price')]\n"
     ]
    }
   ],
   "source": [
    "lowest_sales = lowest_sales(conn)\n",
    "print(lowest_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Correlation Analysis\n",
    "\n",
    "Defines an SQL query to fetch the Population and price columns from the population_vs_price table, ensuring both are cast to DOUBLE for proper numerical analysis.\n",
    "Converts the columns to numeric types, coercing any errors into NaN.\n",
    "Drops any rows with NaN values, ensuring clean data for analysis.\n",
    "Calculates the correlation matrix between the Population and price columns.\n",
    "Visualizes the relationship between population and price using a scatter plot.\n",
    "Returns the correlation matrix after displaying the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def correlation_analysis(conn):\n",
    "    \"\"\"\n",
    "    Fetches data from Databricks and performs correlation analysis.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT CAST(Population AS DOUBLE) AS population, \n",
    "           CAST(price AS DOUBLE) AS price\n",
    "    FROM population_vs_price\n",
    "    WHERE price IS NOT NULL AND Population IS NOT NULL\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    # Ensure numeric conversion (if still needed)\n",
    "    df[\"population\"] = pd.to_numeric(df[\"population\"], errors=\"coerce\")\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop any rows with NaN values after conversion\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Calculate correlation\n",
    "    correlation = df.corr()\n",
    "    print(\"Correlation Matrix:\\n\", correlation)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(x=df['population'], y=df['price'])\n",
    "    plt.title(\"Population vs Median Sales Price\")\n",
    "    plt.xlabel(\"Population\")\n",
    "    plt.ylabel(\"Median Sales Price\")\n",
    "    plt.show()\n",
    "\n",
    "    return correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3509/2059648534.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: \n    SELECT CAST(Population AS DOUBLE) AS population, \n           CAST(price AS DOUBLE) AS price\n    FROM population_vs_price\n    WHERE price IS NOT NULL AND Population IS NOT NULL\n    \n[CAST_INVALID_INPUT] The value 'null' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"ansi_mode\" to \"false\" to bypass this error. SQLSTATE: 22018\n== SQL (line 3, position 12) ==\n           CAST(price AS DOUBLE) AS price\n           ^^^^^^^^^^^^^^^^^^^^^\n\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerOperationError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/client.py:801\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, parameters)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_and_clear_active_result_set()\n\u001b[0;32m--> 801\u001b[0m execute_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrift_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_operation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_handle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marraysize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlz4_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlz4_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cloud_fetch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_cloud_fetch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_result_set \u001b[38;5;241m=\u001b[39m ResultSet(\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection,\n\u001b[1;32m    814\u001b[0m     execute_response,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39muse_cloud_fetch,\n\u001b[1;32m    819\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:919\u001b[0m, in \u001b[0;36mThriftBackend.execute_command\u001b[0;34m(self, operation, session_handle, max_rows, max_bytes, lz4_compression, cursor, use_cloud_fetch, parameters, async_op)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_execute_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:1011\u001b[0m, in \u001b[0;36mThriftBackend._handle_execute_response\u001b[0;34m(self, resp, cursor)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_direct_results_for_error(resp\u001b[38;5;241m.\u001b[39mdirectResults)\n\u001b[0;32m-> 1011\u001b[0m final_operation_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_until_command_done\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperationHandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirectResults\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirectResults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperationStatus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results_message_to_execute_response(resp, final_operation_state)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:833\u001b[0m, in \u001b[0;36mThriftBackend._wait_until_command_done\u001b[0;34m(self, op_handle, initial_operation_status_resp)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_operation_status_resp:\n\u001b[0;32m--> 833\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_command_not_in_error_or_closed_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_operation_status_resp\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m operation_state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    837\u001b[0m     initial_operation_status_resp\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m initial_operation_status_resp\u001b[38;5;241m.\u001b[39moperationState\n\u001b[1;32m    839\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:574\u001b[0m, in \u001b[0;36mThriftBackend._check_command_not_in_error_or_closed_state\u001b[0;34m(self, op_handle, get_operations_resp)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_operations_resp\u001b[38;5;241m.\u001b[39mdisplayMessage:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerOperationError(\n\u001b[1;32m    575\u001b[0m         get_operations_resp\u001b[38;5;241m.\u001b[39mdisplayMessage,\n\u001b[1;32m    576\u001b[0m         {\n\u001b[1;32m    577\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation-id\u001b[39m\u001b[38;5;124m\"\u001b[39m: op_handle\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguid_to_hex_id(op_handle\u001b[38;5;241m.\u001b[39moperationId\u001b[38;5;241m.\u001b[39mguid),\n\u001b[1;32m    579\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnostic-info\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_operations_resp\u001b[38;5;241m.\u001b[39mdiagnosticInfo,\n\u001b[1;32m    580\u001b[0m         },\n\u001b[1;32m    581\u001b[0m     )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mServerOperationError\u001b[0m: [CAST_INVALID_INPUT] The value 'null' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"ansi_mode\" to \"false\" to bypass this error. SQLSTATE: 22018\n== SQL (line 3, position 12) ==\n           CAST(price AS DOUBLE) AS price\n           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2678\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2678\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/client.py:411\u001b[0m, in \u001b[0;36mConnection.rollback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrollback\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransactions are not supported on Databricks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Transactions are not supported on Databricks",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mcorrelation_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(analysis)\n",
      "Cell \u001b[0;32mIn[92], line 15\u001b[0m, in \u001b[0;36mcorrelation_analysis\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mFetches data from Databricks and performs correlation analysis.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mSELECT CAST(Population AS DOUBLE) AS population, \u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m       CAST(price AS DOUBLE) AS price\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mFROM population_vs_price\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mWHERE price IS NOT NULL AND Population IS NOT NULL\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Ensure numeric conversion (if still needed)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2729\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2683\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m     ex \u001b[38;5;241m=\u001b[39m DatabaseError(\n\u001b[1;32m   2681\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124munable to rollback\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2682\u001b[0m     )\n\u001b[0;32m-> 2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql: \n    SELECT CAST(Population AS DOUBLE) AS population, \n           CAST(price AS DOUBLE) AS price\n    FROM population_vs_price\n    WHERE price IS NOT NULL AND Population IS NOT NULL\n    \n[CAST_INVALID_INPUT] The value 'null' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"ansi_mode\" to \"false\" to bypass this error. SQLSTATE: 22018\n== SQL (line 3, position 12) ==\n           CAST(price AS DOUBLE) AS price\n           ^^^^^^^^^^^^^^^^^^^^^\n\nunable to rollback"
     ]
    }
   ],
   "source": [
    "analysis = correlation_analysis(conn)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find average house price for each state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Average Sales Price by State\n",
    "\n",
    "Defines an SQL query to fetch the State and the average price from the population_vs_price table.\n",
    "Groups the data by State and calculates the average sales price for each state.\n",
    "Orders the results by average price in descending order, highlighting the states with the highest average sales prices.\n",
    "Returns the result as a DataFrame containing the State and avg_price for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_price_analysis(conn):\n",
    "    \"\"\"\n",
    "    Finds the average sales price for each state.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT State, AVG(\"price\") AS avg_price \n",
    "    FROM population_vs_price\n",
    "    GROUP BY State\n",
    "    ORDER BY avg_price DESC\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3509/3269785091.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: \n    SELECT State, AVG(\"price\") AS avg_price \n    FROM population_vs_price\n    GROUP BY State\n    ORDER BY avg_price DESC\n    \n[CAST_INVALID_INPUT] The value 'price' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"ansi_mode\" to \"false\" to bypass this error. SQLSTATE: 22018\n== SQL (line 5, position 5) ==\n    ORDER BY avg_price DESC\n    ^^^^^^^^^^^^^^^^^^^^^^^\n\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerOperationError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/client.py:801\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, parameters)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_and_clear_active_result_set()\n\u001b[0;32m--> 801\u001b[0m execute_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrift_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_operation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_handle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marraysize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlz4_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlz4_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cloud_fetch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_cloud_fetch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_result_set \u001b[38;5;241m=\u001b[39m ResultSet(\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection,\n\u001b[1;32m    814\u001b[0m     execute_response,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39muse_cloud_fetch,\n\u001b[1;32m    819\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:919\u001b[0m, in \u001b[0;36mThriftBackend.execute_command\u001b[0;34m(self, operation, session_handle, max_rows, max_bytes, lz4_compression, cursor, use_cloud_fetch, parameters, async_op)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_execute_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:1011\u001b[0m, in \u001b[0;36mThriftBackend._handle_execute_response\u001b[0;34m(self, resp, cursor)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_direct_results_for_error(resp\u001b[38;5;241m.\u001b[39mdirectResults)\n\u001b[0;32m-> 1011\u001b[0m final_operation_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_until_command_done\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperationHandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirectResults\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirectResults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperationStatus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results_message_to_execute_response(resp, final_operation_state)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:833\u001b[0m, in \u001b[0;36mThriftBackend._wait_until_command_done\u001b[0;34m(self, op_handle, initial_operation_status_resp)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_operation_status_resp:\n\u001b[0;32m--> 833\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_command_not_in_error_or_closed_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_operation_status_resp\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m operation_state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    837\u001b[0m     initial_operation_status_resp\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m initial_operation_status_resp\u001b[38;5;241m.\u001b[39moperationState\n\u001b[1;32m    839\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/thrift_backend.py:574\u001b[0m, in \u001b[0;36mThriftBackend._check_command_not_in_error_or_closed_state\u001b[0;34m(self, op_handle, get_operations_resp)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_operations_resp\u001b[38;5;241m.\u001b[39mdisplayMessage:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerOperationError(\n\u001b[1;32m    575\u001b[0m         get_operations_resp\u001b[38;5;241m.\u001b[39mdisplayMessage,\n\u001b[1;32m    576\u001b[0m         {\n\u001b[1;32m    577\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation-id\u001b[39m\u001b[38;5;124m\"\u001b[39m: op_handle\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguid_to_hex_id(op_handle\u001b[38;5;241m.\u001b[39moperationId\u001b[38;5;241m.\u001b[39mguid),\n\u001b[1;32m    579\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnostic-info\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_operations_resp\u001b[38;5;241m.\u001b[39mdiagnosticInfo,\n\u001b[1;32m    580\u001b[0m         },\n\u001b[1;32m    581\u001b[0m     )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mServerOperationError\u001b[0m: [CAST_INVALID_INPUT] The value 'price' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"ansi_mode\" to \"false\" to bypass this error. SQLSTATE: 22018\n== SQL (line 5, position 5) ==\n    ORDER BY avg_price DESC\n    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2678\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2678\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/databricks/sql/client.py:411\u001b[0m, in \u001b[0;36mConnection.rollback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrollback\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransactions are not supported on Databricks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Transactions are not supported on Databricks",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mstate_price_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_analysis)\n",
      "Cell \u001b[0;32mIn[88], line 11\u001b[0m, in \u001b[0;36mstate_price_analysis\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mFinds the average sales price for each state.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mSELECT State, AVG(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) AS avg_price \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mFROM population_vs_price\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mGROUP BY State\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mORDER BY avg_price DESC\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2729\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py:2683\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m     ex \u001b[38;5;241m=\u001b[39m DatabaseError(\n\u001b[1;32m   2681\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124munable to rollback\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2682\u001b[0m     )\n\u001b[0;32m-> 2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql: \n    SELECT State, AVG(\"price\") AS avg_price \n    FROM population_vs_price\n    GROUP BY State\n    ORDER BY avg_price DESC\n    \n[CAST_INVALID_INPUT] The value 'price' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"ansi_mode\" to \"false\" to bypass this error. SQLSTATE: 22018\n== SQL (line 5, position 5) ==\n    ORDER BY avg_price DESC\n    ^^^^^^^^^^^^^^^^^^^^^^^\n\nunable to rollback"
     ]
    }
   ],
   "source": [
    "state_analysis = state_price_analysis(conn)\n",
    "print(state_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
