{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.mgmt.databricks import AzureDatabricksManagementClient\n",
    "from pyspark.sql import SparkSession\n",
    "from mongomock import MongoClient\n",
    "import mongomock\n",
    "# from azure.mgmt.storage import StorageManagementClient\n",
    "# from azure.mgmt.storage.models import StorageAccountCreateParameters, Sku, Kind\n",
    "\n",
    "# from azure.storage.blob import BlobServiceClient\n",
    "import uuid\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "from databricks import sql\n",
    "import subprocess\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, explode, desc, split, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_number = random.randint(1000, 9999)\n",
    "workspace_name = f\"demoworkspacesi{random_number}\"\n",
    "resource_group_name = \"Test\"\n",
    "location = \"eastus\"\n",
    "cluster_name = \"my-databricks-cluster\"\n",
    "managed_resource_group_name = \"test-rg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Azure Credentials\n",
    "\n",
    "Executes the process of loading Azure credentials from a JSON file and returning an authenticated credential object.\n",
    "\n",
    "This function reads the azure_credentials.json file (or a specified file path), extracts required authentication details (tenant_id, client_id, client_secret, and subscription_id), and validates their presence. It then authenticates using ClientSecretCredential, allowing secure access to Azure resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_azure_credentials(credentials_file: str = \"azure_credentials.json\"):\n",
    "    \"\"\"\n",
    "    Loads Azure credentials from a JSON file and returns an authenticated credential object.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to the JSON file containing Azure credentials.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - ClientSecretCredential object for authentication\n",
    "            - Subscription ID as a string\n",
    "    \"\"\"\n",
    "    # Ensure the file exists\n",
    "    if not os.path.exists(credentials_file):\n",
    "        raise FileNotFoundError(f\"The credentials file '{credentials_file}' does not exist.\")\n",
    "    \n",
    "    # Load credentials from the file\n",
    "    with open(credentials_file, \"r\") as file:\n",
    "        creds = json.load(file)\n",
    "    global tenant_id , client_id,client_secret,subscription_id\n",
    "    # Extract required fields\n",
    "    tenant_id = creds.get(\"tenant_id\")\n",
    "    client_id = creds.get(\"client_id\")\n",
    "    client_secret = creds.get(\"client_secret\")\n",
    "    subscription_id = creds.get(\"subscription_id\")\n",
    "    \n",
    "    # Validate required fields\n",
    "    if not all([tenant_id, client_id, client_secret, subscription_id]):\n",
    "        raise ValueError(\"The credentials file is missing one or more required fields: 'tenant_id', 'client_id', 'client_secret', 'subscription_id'.\")\n",
    "    \n",
    "    # Authenticate using ClientSecretCredential\n",
    "    credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)\n",
    "    \n",
    "    return credential, subscription_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "credential, subscription_id = load_azure_credentials(\"azure_credentials.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Mongodb Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Mongo_Resources():\n",
    "\n",
    "    # Initialize mongomock client to simulate MongoDB\n",
    "    mongo_client = mongomock.MongoClient()\n",
    "    db = mongo_client[\"movie_database\"]\n",
    "\n",
    "    titles_collection = db[\"titles\"]\n",
    "    credits_collection = db[\"credits\"]\n",
    "    \n",
    "    # Load Titles data\n",
    "    titles_df = pd.read_csv(\"Titles.csv\")\n",
    "    titles_data = titles_df.to_dict(\"records\")\n",
    "    titles_collection.insert_many(titles_data)\n",
    "\n",
    "    # Load Credits data\n",
    "    credits_df = pd.read_csv(\"Credits.csv\")\n",
    "    credits_data = credits_df.to_dict(\"records\")\n",
    "    credits_collection.insert_many(credits_data)\n",
    "\n",
    "      # Verify insertion by printing counts of documents in each collection\n",
    "    print(f\"Titles count: {titles_collection.count_documents({})}\")\n",
    "    print(f\"Credits count: {credits_collection.count_documents({})}\")\n",
    "\n",
    "    return titles_collection,credits_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles count: 209\n",
      "Credits count: 300\n"
     ]
    }
   ],
   "source": [
    "titles_collection,credits_collection = create_Mongo_Resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction from Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_mongodb(titles_collection,credits_collection):\n",
    "    titles_df, credits_df = None,None\n",
    "    # code starts here\n",
    "    titles_df = pd.DataFrame(list(titles_collection.find({}, {\"_id\": 0,\"id\":1,\"title\":1,\"type\":1,\"genres\":1,\"imdb_score\":1,\"tmdb_score\":1})))\n",
    "    credits_df = pd.DataFrame(list(credits_collection.find({}, {\"_id\": 0})))\n",
    "    # code ends here\n",
    "    return titles_df, credits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles Data:\n",
      "         id                                title   type  \\\n",
      "0  ts300399  Five Came Back: The Reference Films   SHOW   \n",
      "1   tm84618                          Taxi Driver  MOVIE   \n",
      "2  tm154986                          Deliverance  MOVIE   \n",
      "3  tm127384      Monty Python and the Holy Grail  MOVIE   \n",
      "4  tm120801                      The Dirty Dozen  MOVIE   \n",
      "\n",
      "                                 genres  imdb_score  tmdb_score  \n",
      "0                         documentation         NaN         NaN  \n",
      "1                         drama , crime         8.2       8.179  \n",
      "2  drama , action , thriller , european         7.7       7.300  \n",
      "3             fantasy , action , comedy         8.2       7.811  \n",
      "4                          war , action         7.7       7.600  \n",
      "\n",
      "Credits Data:\n",
      "   person_id       id             name                character   role\n",
      "0       3748  tm84618   Robert De Niro            Travis Bickle  ACTOR\n",
      "1      14658  tm84618     Jodie Foster            Iris Steensma  ACTOR\n",
      "2       7064  tm84618    Albert Brooks                      Tom  ACTOR\n",
      "3       3739  tm84618    Harvey Keitel  Matthew 'Sport' Higgins  ACTOR\n",
      "4      48933  tm84618  Cybill Shepherd                    Betsy  ACTOR\n"
     ]
    }
   ],
   "source": [
    "titles_df, credits_df = extract_data_from_mongodb(titles_collection,credits_collection)\n",
    "print(\"Titles Data:\")\n",
    "print(titles_df.head(5))\n",
    "print(\"\\nCredits Data:\")\n",
    "print(credits_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing values in imdb_score and tmdb_score with their respective mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mean_value(titles_df):\n",
    "    # Code starts here\n",
    "    fill_values = {\n",
    "        \"imdb_score\":titles_df[\"imdb_score\"].mean(),\n",
    "        \"tmdb_score\":titles_df[\"tmdb_score\"].mean(),\n",
    "    }\n",
    "    titles_df.fillna(value=fill_values, inplace=True)\n",
    "    # Code ends here\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209 entries, 0 to 208\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          209 non-null    object \n",
      " 1   title       207 non-null    object \n",
      " 2   type        209 non-null    object \n",
      " 3   genres      209 non-null    object \n",
      " 4   imdb_score  209 non-null    float64\n",
      " 5   tmdb_score  209 non-null    float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 9.9+ KB\n"
     ]
    }
   ],
   "source": [
    "titles_df = fill_mean_value(titles_df)\n",
    "titles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve only the titles from the titles_df DataFrame where both imdb_score and tmdb_score are greater than 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_score_morethan_8(titles_df):\n",
    "    # code starts here\n",
    "    titles_df = titles_df[(titles_df[\"imdb_score\"] > 8) & (titles_df[\"tmdb_score\"] > 8)]\n",
    "    # code ends here\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_df = filter_df_score_morethan_8(titles_df)\n",
    "titles_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Databricks Workspace and Retrieving Access Token\n",
    "\n",
    "Executes the process of creating an Azure Databricks workspace and retrieving its access token.\n",
    "\n",
    "This function initializes the Azure Resource Management Client to ensure the resource group exists, then uses the Azure Databricks Management Client to create a Databricks workspace in the specified location. The workspace is configured with a managed resource group and a \"premium\" SKU. Once created, the function returns the workspace name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_databricks_workspace_and_get_token(\n",
    "    credential: credential,\n",
    "    subscription_id: str,\n",
    "    resource_group_name: str,\n",
    "    managed_resource_group_name: str,\n",
    "    workspace_name: str,\n",
    "    location: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Creates a Databricks workspace and returns the workspace URL and access token.\n",
    "\n",
    "    Args:\n",
    "        credential (DefaultAzureCredential): Azure DefaultAzureCredential.\n",
    "        subscription_id (str): Azure subscription ID.\n",
    "        resource_group_name (str): Name of the Azure resource group.\n",
    "        managed_resource_group_name (str): Managed resource group name for Databricks.\n",
    "        workspace_name (str): Name of the Databricks workspace.\n",
    "        location (str): Azure region for the workspace.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the workspace URL and access token.\n",
    "    \"\"\"\n",
    "    # Initialize Resource Management Client\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "\n",
    "    # Create the resource group if it does not exist\n",
    "    print(f\"Ensuring resource group '{resource_group_name}' exists in '{location}'...\")\n",
    "    resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {\"location\": location}\n",
    "    )\n",
    "    print(f\"Resource group '{resource_group_name}' is ready.\")\n",
    "\n",
    "    # Initialize Databricks Management Client\n",
    "    databricks_client = AzureDatabricksManagementClient(credential, subscription_id)\n",
    "\n",
    "    # Construct managed resource group ID\n",
    "    managed_resource_group_id = f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}-managed\"\n",
    "\n",
    "    # Create the Databricks workspace\n",
    "    print(f\"Creating Databricks workspace '{workspace_name}' in '{location}'...\")\n",
    "    workspace = databricks_client.workspaces.begin_create_or_update(\n",
    "        resource_group_name=resource_group_name,\n",
    "        workspace_name=workspace_name,\n",
    "        parameters={\n",
    "            \"location\": location,\n",
    "            \"managed_resource_group_id\": managed_resource_group_id,\n",
    "            \"sku\": {\"name\": \"premium\"}\n",
    "        }\n",
    "    ).result()\n",
    "\n",
    "    print(f\"Databricks workspace '{workspace_name}' created successfully.\")\n",
    "    # print(workspace.workspace_url)\n",
    "    # workspaceurl = workspace.workspace_url\n",
    "    # Retrieve the workspace URL\n",
    "    \n",
    " \n",
    "    return workspace_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring resource group 'Test' exists in 'eastus'...\n",
      "Resource group 'Test' is ready.\n",
      "Creating Databricks workspace 'demoworkspacesi7486' in 'eastus'...\n",
      "Databricks workspace 'demoworkspacesi7486' created successfully.\n"
     ]
    }
   ],
   "source": [
    "workspace_name = create_databricks_workspace_and_get_token(\n",
    "        credential,\n",
    "        subscription_id,\n",
    "        resource_group_name,\n",
    "        managed_resource_group_name,\n",
    "        workspace_name,\n",
    "        location\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving Databricks HTTP Path\n",
    "\n",
    "Fetches the Databricks workspace URL using the Azure Management API.\n",
    "Retrieves the Databricks workspace properties, including the workspace URL.\n",
    "Authenticates and generates an access token for secure API requests.\n",
    "Queries the available SQL Warehouses within the Databricks workspace.\n",
    "Extracts the HTTP path from the first available warehouse.\n",
    "Returns the Databricks server hostname and the HTTP path for database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_databricks_http_path(credentials_file=\"azure_credentials.json\"):\n",
    "    \"\"\"\n",
    "    Retrieves the Databricks HTTP Path for connection.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to Azure credentials file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Databricks Server Hostname, HTTP Path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Databricks workspace URL\n",
    "    workspace_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.Databricks/workspaces/{workspace_name}?api-version=2018-04-01\"\n",
    "\n",
    "    access_token = credential.get_token(\"https://management.azure.com/.default\").token\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(workspace_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve Databricks workspace: {response.text}\")\n",
    "\n",
    "    workspace_data = response.json()\n",
    "    databricks_host = workspace_data[\"properties\"][\"workspaceUrl\"]\n",
    "\n",
    "    # Get SQL Warehouses from Databricks\n",
    "    sql_endpoint_url = f\"https://{databricks_host}/api/2.0/sql/warehouses\"\n",
    "    db_access_token = credential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token  # Databricks token\n",
    "\n",
    "    db_headers = {\n",
    "        \"Authorization\": f\"Bearer {db_access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    sql_response = requests.get(sql_endpoint_url, headers=db_headers)\n",
    "    if sql_response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve SQL Warehouses: {sql_response.text}\")\n",
    "\n",
    "    warehouses = sql_response.json()[\"warehouses\"]\n",
    "    if not warehouses:\n",
    "        raise Exception(\"No SQL Warehouses found in Databricks.\")\n",
    "\n",
    "    warehouse = warehouses[0]  # Selecting the first available warehouse\n",
    "    http_path = warehouse[\"odbc_params\"][\"path\"]\n",
    "\n",
    "    return databricks_host, http_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adb-134158506596936.16.azuredatabricks.net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/sql/1.0/warehouses/59e48023dd645b8d'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databricks_url, databricks_http_path = get_databricks_http_path()\n",
    "print(databricks_url)\n",
    "databricks_http_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create token of the databricks using the below steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "databricks_token = \"dapi69e4f606d12b6c6f9e2ff579a4725926-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Databricks Cluster\n",
    "\n",
    "Constructs the Databricks API endpoint for cluster creation.\n",
    "Formats the workspace URL to ensure proper API request formatting.\n",
    "Sets up authentication headers using the provided access token.\n",
    "Defines cluster configuration, including the name, Spark version, node type, and auto-termination settings.\n",
    "Sends a POST request to the Databricks API to create the cluster.\n",
    "Handles API responses, printing success messages or error details based on the response status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def create_databricks_cluster(workspaceurl, access_token: str):\n",
    "    \n",
    "    # Validate and format the workspace URL\n",
    "    \n",
    "    # API endpoint for cluster creation\n",
    "    api_endpoint = \"/api/2.1/clusters/create\"\n",
    "    api_url = workspaceurl.rstrip(\"/\") + api_endpoint  # Ensure no trailing slash in URL\n",
    "\n",
    "    # Headers for authentication\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # Cluster configuration\n",
    "    cluster_config = {\n",
    "        \"cluster_name\": \"democluster\",\n",
    "        \"spark_version\": \"16.1.x-scala2.12\",  # Replace with your required version\n",
    "        \"node_type_id\": \"Standard_DS3_v2\",   # Choose the appropriate node type\n",
    "        \"autotermination_minutes\": 30,\n",
    "        \"num_workers\": 2\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send POST request to create the cluster\n",
    "        response = requests.post(api_url, headers=headers, json=cluster_config)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Cluster created successfully!\")\n",
    "            print(\"Cluster Details:\", response.json())\n",
    "        else:\n",
    "            print(f\"Failed to create cluster. HTTP Status Code: {response.status_code}\")\n",
    "            print(\"Error Response:\", response.text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"An error occurred while connecting to Databricks API:\")\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster created successfully!\n",
      "Cluster Details: {'cluster_id': '0204-193804-q9nzsuk3'}\n"
     ]
    }
   ],
   "source": [
    "access_token = databricks_token\n",
    "workspaceurl = \"https://\"+databricks_url\n",
    "#\"https://\"+databricks_url\n",
    "create_databricks_cluster(workspaceurl, access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing a Databricks SQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_databricks_connection():\n",
    "    \"\"\"\n",
    "    Establish and return a Databricks SQL connection.\n",
    "    Ensure to close the connection after use.\n",
    "\n",
    "    Returns:\n",
    "        conn (sql.Connection): Databricks SQL connection object.\n",
    "    \"\"\"\n",
    "    # Set up connection details\n",
    "    DATABRICKS_SERVER_HOSTNAME = workspaceurl  # Replace with your Databricks workspace URL\n",
    "    DATABRICKS_HTTP_PATH = databricks_http_path # Replace with your warehouse's HTTP Path\n",
    "    DATABRICKS_ACCESS_TOKEN = access_token  # Replace with your access token\n",
    "\n",
    "    # Establish connection\n",
    "    conn = sql.connect(\n",
    "        server_hostname=DATABRICKS_SERVER_HOSTNAME,\n",
    "        http_path=DATABRICKS_HTTP_PATH,\n",
    "        access_token=DATABRICKS_ACCESS_TOKEN\n",
    "    )\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<databricks.sql.client.Connection at 0x753b12d49930>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global conn\n",
    "conn = get_databricks_connection()\n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_dataframe(titles_df, credits_df):\n",
    "    titles_spark_df, credits_spark_df = None,None\n",
    "     # code starts here\n",
    "\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder.appName(\"DataAnalysis\").getOrCreate()\n",
    "\n",
    "    # convert the titles and credits dataframes to spark dataframes\n",
    "    titles_spark_df = spark.createDataFrame(titles_df)\n",
    "    credits_spark_df = spark.createDataFrame(credits_df)  \n",
    "    \n",
    "    # code ends here\n",
    "    return titles_spark_df, credits_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/04 19:43:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "titles_spark_df, credits_spark_df = create_spark_dataframe(titles_df, credits_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation\n",
    "Processing genre column data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_genres_separated_spark_df(titles_spark_df):\n",
    "    titles_genres_separated_spark_df = None\n",
    "\n",
    "    # Code starts here\n",
    "    # Step 1: Split the genres column into an array (handling spaces correctly)\n",
    "    df_titles = titles_spark_df.withColumn(\"genres_array\", split(col(\"genres\"), \"\\\\s*,\\\\s*\"))\n",
    "\n",
    "    # Step 2: Explode the array into multiple rows\n",
    "    df_exploded = df_titles.withColumn(\"genre\", explode(col(\"genres_array\")))\n",
    "\n",
    "    # Step 3: Remove 'NA' , empty strings, and empty lists (\"[]\") after explosion\n",
    "    titles_genres_separated_spark_df = df_exploded.filter((col(\"genre\") != \"NA\") & (col(\"genre\") != \"\") & (col(\"genre\") != \"[]\"))\n",
    "    # Step 4: Trim spaces (this ensures no leading/trailing spaces remain)\n",
    "    titles_genres_separated_spark_df = titles_genres_separated_spark_df.withColumn(\"genre\", trim(col(\"genre\")))\n",
    "    titles_genres_separated_spark_df = titles_genres_separated_spark_df.select(\"id\", \"title\", \"type\", \"genre\",\"imdb_score\",\"tmdb_score\")\n",
    "\n",
    "    # Code ends here\n",
    "    return titles_genres_separated_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------+----------+----------+\n",
      "|      id|               title| type|   genre|imdb_score|tmdb_score|\n",
      "+--------+--------------------+-----+--------+----------+----------+\n",
      "| tm84618|         Taxi Driver|MOVIE|   drama|       8.2|     8.179|\n",
      "| tm84618|         Taxi Driver|MOVIE|   crime|       8.2|     8.179|\n",
      "| ts22164|Monty Python's Fl...| SHOW|  comedy|       8.8|     8.306|\n",
      "| ts22164|Monty Python's Fl...| SHOW|european|       8.8|     8.306|\n",
      "| tm81728|            The Land|MOVIE|   drama|       8.1|       8.5|\n",
      "| ts20681|            Seinfeld| SHOW|  comedy|       8.9|     8.301|\n",
      "|tm155787|          GoodFellas|MOVIE|   drama|       8.7|     8.463|\n",
      "|tm155787|          GoodFellas|MOVIE|   crime|       8.7|     8.463|\n",
      "| tm22327|   Full Metal Jacket|MOVIE|     war|       8.3|       8.1|\n",
      "| tm22327|   Full Metal Jacket|MOVIE|   drama|       8.3|       8.1|\n",
      "|tm180542|Once Upon a Time ...|MOVIE|   crime|       8.3|     8.453|\n",
      "|tm180542|Once Upon a Time ...|MOVIE|   drama|       8.3|     8.453|\n",
      "|tm180542|Once Upon a Time ...|MOVIE|european|       8.3|     8.453|\n",
      "| ts22176|       Stargate SG-1| SHOW|   scifi|       8.4|       8.3|\n",
      "| ts22176|       Stargate SG-1| SHOW|   drama|       8.4|       8.3|\n",
      "| ts22176|       Stargate SG-1| SHOW|  action|       8.4|       8.3|\n",
      "| tm15897|               Se7en|MOVIE|   crime|       8.6|     8.352|\n",
      "| tm15897|               Se7en|MOVIE|thriller|       8.6|     8.352|\n",
      "| tm15897|               Se7en|MOVIE|   drama|       8.6|     8.352|\n",
      "|tm122434|        Forrest Gump|MOVIE|   drama|       8.8|     8.478|\n",
      "+--------+--------------------+-----+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles_genres_separated_spark_df = get_titles_genres_separated_spark_df(titles_spark_df)\n",
    "titles_genres_separated_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_genres_separated_spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the titles_genres_separated_spark_df with the credits DataFrame to include actor details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_data(titles_genres_separated_spark_df,credits_spark_df):\n",
    "    transformed_data = None\n",
    "    # code starts here\n",
    "    # Perform an INNER JOIN on the 'id' column (title ID)\n",
    "    df_joined = titles_genres_separated_spark_df.join(credits_spark_df, titles_genres_separated_spark_df.id == credits_spark_df.id, \"inner\")\n",
    "\n",
    "    # Select relevant columns: title, type, genre, actor name, and role\n",
    "    transformed_data = df_joined.select(\n",
    "        titles_genres_separated_spark_df.id, \n",
    "        titles_genres_separated_spark_df.title, \n",
    "        titles_genres_separated_spark_df.type, \n",
    "        titles_genres_separated_spark_df.genre, \n",
    "        credits_spark_df.name.alias(\"actor_name\"), \n",
    "        credits_spark_df.role,\n",
    "        titles_genres_separated_spark_df.imdb_score,\n",
    "        titles_genres_separated_spark_df.tmdb_score\n",
    "    )\n",
    "    # Code ends here\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+-----+------------------+-----+----------+----------+\n",
      "|     id|      title| type|genre|        actor_name| role|imdb_score|tmdb_score|\n",
      "+-------+-----------+-----+-----+------------------+-----+----------+----------+\n",
      "|tm84618|Taxi Driver|MOVIE|drama|    Robert De Niro|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|      Jodie Foster|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|     Albert Brooks|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|     Harvey Keitel|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|   Cybill Shepherd|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|       Peter Boyle|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|    Leonard Harris|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|    Diahnne Abbott|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|       Gino Ardito|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|   Martin Scorsese|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|     Murray Moston|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|     Richard Higgs|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|       Bill Minkin|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|        Bob Maroff|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|       Victor Argo|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|       Joe Spinell|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|Robinson Frank Adu|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|    Brenda Dickson|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|    Norman Matlock|ACTOR|       8.2|     8.179|\n",
      "|tm84618|Taxi Driver|MOVIE|drama|     Harry Northup|ACTOR|       8.2|     8.179|\n",
      "+-------+-----------+-----+-----+------------------+-----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data = get_transformed_data(titles_genres_separated_spark_df,credits_spark_df)\n",
    "transformed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
